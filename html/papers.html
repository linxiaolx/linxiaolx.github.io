<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Lin Xiao &ndash; Papers</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Lin Xiao</div>
<div class="menu-item"><a href="../index.html">home</a></div>
<div class="menu-item"><a href="../html/bio.html">biography</a></div>
<div class="menu-category">Research</div>
<div class="menu-item"><a href="../html/papers.html" class="current">papers</a></div>
<div class="menu-item"><a href="../html/talks.html">talks</a></div>
<div class="menu-item"><a href="../html/software.html">software</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Lin Xiao &ndash; Papers</h1>
</div>
<p><a href="https://scholar.google.com/citations?user=vK0-CDcAAAAJ&amp;hl=en">Google scholar page</a> </p>
<h3>2021</h3>
<ul>
<li><p><a href="https://link.springer.com/article/10.1007%2Fs11590-021-01748-7">On self-concordant barriers for generalized power cones</a> <br />
Scott Roy and Lin Xiao <br />
<i>Optimization Letters</i>, published online, June, 2021.</p>
</li>
</ul>
<ul>
<li><p><a href="https://jmlr.org/beta/papers/v22/20-821.html">From low probability to high confidence in stochastic convex optimization</a> <br />
Damek Davis, Dmitriy Drusvyatskiy, Lin Xiao, and Junyu Zhang <br />
<i>Journal of Machine Learning Research</i>, 22(49):1-38, 2021.</p>
</li>
</ul>
<ul>
<li><p><a href="https://link.springer.com/article/10.1007%2Fs10589-021-00273-8">Accelerated Bregman proximal gradient methods for relatively smooth convex optimization</a> <br />
Filip Hanely, Peter Richtarik, and Lin Xiao <br />
<i>Computational Optimization and Applications</i>, 79:405-440, 2021.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1908.11468">MultiLevel Composite Stochastic Optimization via Nested Variance Reduction</a> <br />
Junyu Zhang and Lin Xiao <br />
<i>SIAM Journal on Optimization</i>, 31(2):1131-1157, 2021.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1811.11637">Adaptive stochastic variance reduction for subsampled Newton method with cubic regularization</a> <br />
Junyu Zhang, Lin Xiao, and Shuzhong Zhang <br />
<i>INFORMS Journal on Optimization</i>, to appear, 2021.</p>
</li>
</ul>
<h3>2020</h3>
<ul>
<li><p><a href="https://arxiv.org/abs/2011.11173">Stochastic optimization with decision-dependent distributions</a> <br />
Dmitriy Drusvyatskiy and Lin Xiao  <br />
<i>arXiv preprint</i>, 2020.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2002.10597">Statistical Adaptive Stochastic Gradient Methods</a> <br />
Pengchuan Zhang, Hunter Lang, Qiang Liu, and Lin Xiao <br />
<i>arXiv preprint</i>, 2020.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2004.04357">Stochastic variance-reduced prox-linear algorithms for nonconvex composite optimization</a> <br />
Junyu Zhang and Lin Xiao  <br />
<i>arXiv preprint</i>, 2020.</p>
</li>
</ul>
<ul>
<li><p><a href="http://proceedings.mlr.press/v119/hendrikx20a.html">Statistically Preconditioned Accelerated Gradient Method for Distributed Optimization</a> <br />
Hadrien Hendrikx, Lin Xiao, Sebastien Bubeck, Francis Bach, Laurent Massoulie <br />
<i>Proceedings of the 37th International Conference on Machine Learning</i>, PMLR 119:4203-4227, 2020.</p>
</li>
</ul>
<h3>2019</h3>
<ul>
<li><p><a href="https://jmlr.org/papers/v20/17-608.html">DSCOVR: Randomized Primal-Dual Block Coordinate Algorithms for Asynchronous Distributed Optimization</a> <br />
Lin Xiao, Adams Wei Yu, Qihang Lin, Weizhu Chen <br />
<i>Journal of Machine Learning Research</i>, 20(43):1−58, 2019.</p>
</li>
</ul>
<ul>
<li><p><a href="https://papers.nips.cc/paper/2019/hash/e1054bf2d703bca1e8fe101d3ac5efcd-Abstract.html">Using Statistics to Automate Stochastic Optimization</a> <br />
Hunter Lang, Lin Xiao, Pengchuan Zhang <br />
<i>Advances in Neural Information Processing Systems 32</i> (NeurIPS 2019).</p>
</li>
</ul>
<ul>
<li><p><a href="https://proceedings.neurips.cc/paper/2019/hash/4eff0720836a198b6174eecf02cbfdbf-Abstract.html">Understanding the Role of Momentum in Stochastic Gradient Methods</a> <br />
Igor Gitman, Hunter Lang, Pengchuan Zhang, Lin Xiao <br />
<i>Advances in Neural Information Processing Systems 32</i> (NeurIPS 2019).</p>
</li>
</ul>
<ul>
<li><p><a href="https://proceedings.neurips.cc/paper/2019/hash/a68259547f3d25ab3c0a5c0adb4e3498-Abstract.html">Stochastic Composite Gradient Method with Incremental Variance Reduction</a> <br />
Junyu Zhang, Lin Xiao
<i>Advances in Neural Information Processing Systems 32</i> (NeurIPS 2019).</p>
</li>
</ul>
<h3>2018</h3>
<ul>
<li><p><a href="https://papers.nips.cc/paper/2018/hash/6aaba9a124857622930ca4e50f5afed2-Abstract.html">Coupled Variational Bayes via Optimization Embedding</a> <br />
Bo Dai, Hanjun Dai, Niao He, Weiyang Liu, Zhen Liu, Jianshu Chen, Lin Xiao, Le Song <br />
<i>Advances in Neural Information Processing Systems 31</i> (NeurIPS 2018).</p>
</li>
</ul>
<ul>
<li><p><a href="https://papers.nips.cc/paper/2018/hash/03b2ceb73723f8b53cd533e4fba898ee-Abstract.html">Learning SMaLL Predictors</a> <br />
Vikas Garg, Ofer Dekel, Lin Xiao <br />
<i>Advances in Neural Information Processing Systems 31</i> (NeurIPS 2018).</p>
</li>
</ul>
<ul>
<li><p><a href="http://proceedings.mlr.press/v80/dai18c.html">SBEED: Convergent Reinforcement Learning with Nonlinear Function Approximation</a> <br />
Bo Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, Le Song <br />
<i>Proceedings of the 35th International Conference on Machine Learning</i>, PMLR 80:1125-1134, 2018.</p>
</li>
</ul>
<ul>
<li><p><a href="http://auai.org/uai2018/proceedings/papers/250.pdf">Sparse Multi-Prototype Classification</a> <br />
Vikas K. Garg, Lin Xiao, Ofer Dekel <br />
<i>Proceedings of the Conference on Uncetainty in Artifical Intelligence (UAI)</i>, 2018.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1501.00263">Communication-Efficient Distributed Optimization of Self-Concordant Empirical Loss</a> <br />
Yuchen Zhang and Lin Xiao <br />
<i>Large-Scale and Distributed Optimization</i>, Giselsson and Rantzer (Editors), pages 289-341, Springer, 2018.</p>
</li>
</ul>
<h3>2017</h3>
<ul>
<li><p><a href="https://jmlr.org/beta/papers/v18/16-568.html">Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization</a> <br />
Yuchen Zhang and Lin Xiao <br />
<i>Journal of Machine Learning Research</i>, 18(84):1-42, 2017.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1507.04734">Variational Gram functions: convex analysis and optimization</a> <br />
Amin Jalali, Maryam Fazel, Lin Xiao <br />
<i>SIAM Journal on Optimization</i>, 27(4):634-2661, 2017.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1306.5918">A Randomized Nonmonotone Block Proximal Gradient Method for a Class of Structured Nonlinear Programming</a> <br />
Zhaosong Lu and Lin Xiao <br />
<i>SIAM Journal on Numerical Analysis</i>, 55(6):2930-2955, 2017.</p>
</li>
</ul>
<ul>
<li><p><a href="https://papers.nips.cc/paper/2017/hash/c0e90532fb42ac6de18e25e95db73047-Abstract.html">Q-LDA: Uncovering Latent Patterns in Text-based Sequential Decision Processes</a> <br />
Jianshu Chen, Chong Wang, Lin Xiao, Ji He, Lihong Li, Li Deng <br />
<i>Advances in Neural Information Processing Systems 30</i> (NIPS 2017).</p>
</li>
</ul>
<ul>
<li><p><a href="http://proceedings.mlr.press/v70/du17a.html">Stochastic Variance Reduction Methods for Policy Evaluation</a> <br />
Simon S. Du, Jianshu Chen, Lihong Li, Lin Xiao, Dengyong Zhou <br />
<i>Proceedings of the 34th International Conference on Machine Learning</i>, PMLR 70:1049-1058, 2017.</p>
</li>
</ul>
<ul>
<li><p><a href="http://proceedings.mlr.press/v70/wang17l.html">Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms</a> <br />
Jialei Wang and Lin Xiao <br />
<i>Proceedings of the 34th International Conference on Machine Learning</i>, PMLR 70:3694-3702, 2017.</p>
</li>
</ul>
<h3>2015</h3>
<ul>
<li><p><a href="https://epubs.siam.org/doi/abs/10.1137/141000270">An Accelerated Randomized Proximal Coordinate Gradient Method and its Application to Regularized Empirical Risk Minimization</a> <br />
Qihang Lin, Zhaosong Lu, and Lin Xiao <br />
<i>SIAM Journal on Optimization</i>, 25(4):2244–2273, 2015.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1305.4723">On the Complexity Analysis of Randomized Block-Coordinate Descent Methods</a> <br />
Zhaosong Lu and Lin Xiao <br />
<i>Mathematical Programming</i>, Series A, 152(1-2):15-642, 2015.</p>
</li>
</ul>
<ul>
<li><p><a href="https://link.springer.com/article/10.1007/s10589-014-9694-4">An adaptive accelerated proximal gradient method and its homotopy continuation for sparse optimization</a> <br />
Qihang Lin and Lin Xiao <br />
<i>Computational Optimization and Applications</i>, 60(3):633-674, 2015.</p>
</li>
</ul>
<ul>
<li><p><a href="https://papers.nips.cc/paper/2015/hash/4ca82782c5372a547c104929f03fe7a9-Abstract.html">End-to-end Learning of LDA by Mirror-Descent Back Propagation over a Deep Architecture</a> <br />
Jianshu Chen, Ji He, Yelong Shen, Lin Xiao, Xiaodong He, Jianfeng Gao, Xinying Song, Li Deng <br />
<i>Advances in Neural Information Processing Systems 28</i> (NIPS 2015).</p>
</li>
</ul>
<ul>
<li><p><a href="https://dl.acm.org/doi/10.1145/2783258.2783412">Scaling Up Stochastic Dual Coordinate Ascent</a> <br />
Kenneth Tran, Saghar Hosseini, Lin Xiao, Thomas Finley, Mikhail Bilenko <br />
<i>Proceedings of the 21st ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</i>, 2015.</p>
</li>
</ul>
<ul>
<li><p><a href="http://proceedings.mlr.press/v37/zhanga15.html">Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization</a> <br />
Yuchen Zhang and Lin Xiao <br />
<i>Proceedings of the 32nd International Conference on Machine Learning</i>, PMLR 37:353-361, 2015.</p>
</li>
</ul>
<ul>
<li><p><a href="http://proceedings.mlr.press/v37/zhangb15.html">DiSCO: Distributed Optimization for Self-Concordant Empirical Loss</a> <br />
Yuchen Zhang and Lin Xiao <br />
<i>Proceedings of the 32nd International Conference on Machine Learning</i>, PMLR 37:362-370, 2015.</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2021-07-27 22:58:34 PDT, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
