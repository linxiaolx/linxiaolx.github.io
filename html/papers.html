<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Lin Xiao &ndash; Papers</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Lin Xiao</div>
<div class="menu-item"><a href="../index.html">home</a></div>
<div class="menu-item"><a href="../html/bio.html">biography</a></div>
<div class="menu-item"><a href="../html/papers.html" class="current">papers</a></div>
<div class="menu-item"><a href="../html/talks.html">talks</a></div>
<div class="menu-item"><a href="../html/software.html">software</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Lin Xiao &ndash; Papers</h1>
</div>
<p><a href="https://scholar.google.com/citations?user=vK0-CDcAAAAJ&amp;hl=en" target=&ldquo;blank&rdquo;>Google scholar page</a> 
</p>
<h3>2025</h3>
<ul>
<li><p><a href="https://arxiv.org/abs/2507.08963" target=&ldquo;blank&rdquo;>Stochastic Approximation with Block Coordinate Optimal Stepsizes</a> <br />
Tao Jiang and Lin Xiao <br />
arXiv preprint, 2025.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2508.11112" target=&ldquo;blank&rdquo;>Quantization through Piecewise-Affine Regularization: Optimization and Statistical Guarantees</a> <br />
Jianhao Ma and Lin Xiao <br />
arXiv preprint, 2025.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://openreview.net/pdf?id=A5Y8Uh5Szl" target=&ldquo;blank&rdquo;>Exploration from a Primal-Dual Lens: Value-Incentivized Actor-Critic Methods for Sample-Efficient Online RL</a> <br />
Tong Yang, Bo Dai, Lin Xiao, Yuejie Chi <br />
NeurIPS 2025.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://openreview.net/pdf?id=PMSNd8xTHp" target=&ldquo;blank&rdquo;>ParetoQ: Improving Scaling Laws in Extremely Low-bit LLM Quantization</a> <br />
Zechun Liu, Changsheng Zhao, Hanxian Huang, Sijia Chen, Jing Zhang, Jiawei Zhao, Scott Roy, Lisa Jin, Yunyang Xiong, Yangyang Shi, Lin Xiao, Yuandong Tian, Bilge Soran, Raghuraman Krishnamoorthi, Tijmen Blankevoort, Vikas Chandra <br />
NeurIPS 2025.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://proceedings.mlr.press/v267/jin25e.html" target=&ldquo;blank&rdquo;>PARQ: Piecewise-Affine Regularized Quantization</a> <br />
Lisa Jin, Jianhao Ma, Zechun Liu, Andrey Gromov, Aaron Defazio, Lin Xiao <br />
ICML 2025.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://proceedings.mlr.press/v267/yang25j.html" target=&ldquo;blank&rdquo;>Incentivize without Bonus: Provably Efficient Model-based Online Multi-agent RL for Markov Games</a> <br />
Tong Yang, Bo Dai, Lin Xiao, Yuejie Chi <br />
ICML 2025.
</p>
</li>
</ul>
<h3>2024</h3>
<ul>
<li><p><a href="https://projecteuclid.org/journals/annals-of-statistics/volume-52/issue-6/Noisy-recovery-from-random-linear-observations--Sharp-minimax-rates/10.1214/24-AOS2446.short" target=&ldquo;blank&rdquo;>Noisy recovery from random linear observations: Sharp minimax rates under elliptical constraints</a> <br />
Reese Pathak, Martin J. Wainwright, Lin Xiao <br />
<i>Annals of Statistics</i>, 52(6):2816-2850, December 2024. 
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2410.01249" target=&ldquo;blank&rdquo;>Dual Approximation Policy Optimization</a> <br />
Zhihan Xiong, Maryam Fazel and Lin Xiao <br />
arXiv preprint, 2024.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2407.04358" target=&ldquo;blank&rdquo;>An Adaptive Stochastic Gradient Method with Non-negative Gauss-Newton Stepsizes</a> <br />
Antonio Orvieto and Lin Xiao <br />
arXiv preprint, 2024.
</p>
</li>
</ul>
<h3>2023</h3>
<ul>
<li><p><a href="https://pubsonline.informs.org/doi/10.1287/moor.2022.1287" target=&ldquo;blank&rdquo;>Stochastic Optimization with Decision-Dependent Distributions</a> <br />
Dmitriy Drusvyatskiy and Lin Xiao <br />
<i>Mathematics of Operations Research</i>, 48(2):954-998, 2023.
</p>
</li>
</ul>
<h3>2022</h3>
<ul>
<li><p><a href="https://www.jmlr.org/papers/v23/22-0056.html" target=&ldquo;blank&rdquo;>On the convergence rates of policy gradient methods</a> <br />
Lin Xiao <br />
<i>Journal of Machine Learning Research</i>, 23(282):1-36, 2022.
</p>
</li>
</ul>
<h3>2021</h3>
<ul>
<li><p><a href="https://link.springer.com/article/10.1007%2Fs11590-021-01748-7" target=&ldquo;blank&rdquo;>On self-concordant barriers for generalized power cones</a> <br />
Scott Roy and Lin Xiao <br />
<i>Optimization Letters</i>, published online, June, 2021.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://jmlr.org/beta/papers/v22/20-821.html" target=&ldquo;blank&rdquo;>From low probability to high confidence in stochastic convex optimization</a> <br />
Damek Davis, Dmitriy Drusvyatskiy, Lin Xiao, and Junyu Zhang <br />
<i>Journal of Machine Learning Research</i>, 22(49):1-38, 2021.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://link.springer.com/article/10.1007%2Fs10589-021-00273-8" target=&ldquo;blank&rdquo;>Accelerated Bregman proximal gradient methods for relatively smooth convex optimization</a> <br />
Filip Hanely, Peter Richtarik, and Lin Xiao <br />
<i>Computational Optimization and Applications</i>, 79:405-440, 2021.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1908.11468" target=&ldquo;blank&rdquo;>MultiLevel Composite Stochastic Optimization via Nested Variance Reduction</a> <br />
Junyu Zhang and Lin Xiao <br />
<i>SIAM Journal on Optimization</i>, 31(2):1131-1157, 2021.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1811.11637" target=&ldquo;blank&rdquo;>Adaptive stochastic variance reduction for subsampled Newton method with cubic regularization</a> <br />
Junyu Zhang, Lin Xiao, and Shuzhong Zhang <br />
<i>INFORMS Journal on Optimization</i>, to appear, 2021.
</p>
</li>
</ul>
<h3>2020</h3>
<ul>
<li><p><a href="https://arxiv.org/abs/2011.11173" target=&ldquo;blank&rdquo;>Stochastic optimization with decision-dependent distributions</a> <br />
Dmitriy Drusvyatskiy and Lin Xiao  <br />
<i>arXiv preprint</i>, 2020.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2002.10597" target=&ldquo;blank&rdquo;>Statistical Adaptive Stochastic Gradient Methods</a> <br />
Pengchuan Zhang, Hunter Lang, Qiang Liu, and Lin Xiao <br />
<i>arXiv preprint</i>, 2020.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2004.04357" target=&ldquo;blank&rdquo;>Stochastic variance-reduced prox-linear algorithms for nonconvex composite optimization</a> <br />
Junyu Zhang and Lin Xiao  <br />
<i>arXiv preprint</i>, 2020.
</p>
</li>
</ul>
<ul>
<li><p><a href="http://proceedings.mlr.press/v119/hendrikx20a.html" target=&ldquo;blank&rdquo;>Statistically Preconditioned Accelerated Gradient Method for Distributed Optimization</a> <br />
Hadrien Hendrikx, Lin Xiao, Sebastien Bubeck, Francis Bach, Laurent Massoulie <br />
<i>Proceedings of the 37th International Conference on Machine Learning</i>, PMLR 119:4203-4227, 2020.
</p>
</li>
</ul>
<h3>2019</h3>
<ul>
<li><p><a href="https://jmlr.org/papers/v20/17-608.html" target=&ldquo;blank&rdquo;>DSCOVR: Randomized Primal-Dual Block Coordinate Algorithms for Asynchronous Distributed Optimization</a> <br />
Lin Xiao, Adams Wei Yu, Qihang Lin, Weizhu Chen <br />
<i>Journal of Machine Learning Research</i>, 20(43):1−58, 2019.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://papers.nips.cc/paper/2019/hash/e1054bf2d703bca1e8fe101d3ac5efcd-Abstract.html" target=&ldquo;blank&rdquo;>Using Statistics to Automate Stochastic Optimization</a> <br />
Hunter Lang, Lin Xiao, Pengchuan Zhang <br />
<i>Advances in Neural Information Processing Systems 32</i> (NeurIPS 2019).
</p>
</li>
</ul>
<ul>
<li><p><a href="https://proceedings.neurips.cc/paper/2019/hash/4eff0720836a198b6174eecf02cbfdbf-Abstract.html" target=&ldquo;blank&rdquo;>Understanding the Role of Momentum in Stochastic Gradient Methods</a> <br />
Igor Gitman, Hunter Lang, Pengchuan Zhang, Lin Xiao <br />
<i>Advances in Neural Information Processing Systems 32</i> (NeurIPS 2019).
</p>
</li>
</ul>
<ul>
<li><p><a href="https://proceedings.neurips.cc/paper/2019/hash/a68259547f3d25ab3c0a5c0adb4e3498-Abstract.html" target=&ldquo;blank&rdquo;>Stochastic Composite Gradient Method with Incremental Variance Reduction</a> <br />
Junyu Zhang, Lin Xiao
<i>Advances in Neural Information Processing Systems 32</i> (NeurIPS 2019).
</p>
</li>
</ul>
<h3>2018</h3>
<ul>
<li><p><a href="https://papers.nips.cc/paper/2018/hash/6aaba9a124857622930ca4e50f5afed2-Abstract.html" target=&ldquo;blank&rdquo;>Coupled Variational Bayes via Optimization Embedding</a> <br />
Bo Dai, Hanjun Dai, Niao He, Weiyang Liu, Zhen Liu, Jianshu Chen, Lin Xiao, Le Song <br />
<i>Advances in Neural Information Processing Systems 31</i> (NeurIPS 2018).
</p>
</li>
</ul>
<ul>
<li><p><a href="https://papers.nips.cc/paper/2018/hash/03b2ceb73723f8b53cd533e4fba898ee-Abstract.html" target=&ldquo;blank&rdquo;>Learning SMaLL Predictors</a> <br />
Vikas Garg, Ofer Dekel, Lin Xiao <br />
<i>Advances in Neural Information Processing Systems 31</i> (NeurIPS 2018).
</p>
</li>
</ul>
<ul>
<li><p><a href="http://proceedings.mlr.press/v80/dai18c.html" target=&ldquo;blank&rdquo;>SBEED: Convergent Reinforcement Learning with Nonlinear Function Approximation</a> <br />
Bo Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, Le Song <br />
<i>Proceedings of the 35th International Conference on Machine Learning</i>, PMLR 80:1125-1134, 2018.
</p>
</li>
</ul>
<ul>
<li><p><a href="http://auai.org/uai2018/proceedings/papers/250.pdf" target=&ldquo;blank&rdquo;>Sparse Multi-Prototype Classification</a> <br />
Vikas K. Garg, Lin Xiao, Ofer Dekel <br />
<i>Proceedings of the Conference on Uncetainty in Artifical Intelligence (UAI)</i>, 2018.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1501.00263" target=&ldquo;blank&rdquo;>Communication-Efficient Distributed Optimization of Self-Concordant Empirical Loss</a> <br />
Yuchen Zhang and Lin Xiao <br />
<i>Large-Scale and Distributed Optimization</i>, Giselsson and Rantzer (Editors), pages 289-341, Springer, 2018.
</p>
</li>
</ul>
<h3>2017</h3>
<ul>
<li><p><a href="https://jmlr.org/beta/papers/v18/16-568.html" target=&ldquo;blank&rdquo;>Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization</a> <br />
Yuchen Zhang and Lin Xiao <br />
<i>Journal of Machine Learning Research</i>, 18(84):1-42, 2017.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1507.04734" target=&ldquo;blank&rdquo;>Variational Gram functions: convex analysis and optimization</a> <br />
Amin Jalali, Maryam Fazel, Lin Xiao <br />
<i>SIAM Journal on Optimization</i>, 27(4):634-2661, 2017.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1306.5918" target=&ldquo;blank&rdquo;>A Randomized Nonmonotone Block Proximal Gradient Method for a Class of Structured Nonlinear Programming</a> <br />
Zhaosong Lu and Lin Xiao <br />
<i>SIAM Journal on Numerical Analysis</i>, 55(6):2930-2955, 2017.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://papers.nips.cc/paper/2017/hash/c0e90532fb42ac6de18e25e95db73047-Abstract.html" target=&ldquo;blank&rdquo;>Q-LDA: Uncovering Latent Patterns in Text-based Sequential Decision Processes</a> <br />
Jianshu Chen, Chong Wang, Lin Xiao, Ji He, Lihong Li, Li Deng <br />
<i>Advances in Neural Information Processing Systems 30</i> (NIPS 2017).
</p>
</li>
</ul>
<ul>
<li><p><a href="http://proceedings.mlr.press/v70/du17a.html" target=&ldquo;blank&rdquo;>Stochastic Variance Reduction Methods for Policy Evaluation</a> <br />
Simon S. Du, Jianshu Chen, Lihong Li, Lin Xiao, Dengyong Zhou <br />
<i>Proceedings of the 34th International Conference on Machine Learning</i>, PMLR 70:1049-1058, 2017.
</p>
</li>
</ul>
<ul>
<li><p><a href="http://proceedings.mlr.press/v70/wang17l.html" target=&ldquo;blank&rdquo;>Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms</a> <br />
Jialei Wang and Lin Xiao <br />
<i>Proceedings of the 34th International Conference on Machine Learning</i>, PMLR 70:3694-3702, 2017.
</p>
</li>
</ul>
<h3>2015</h3>
<ul>
<li><p><a href="https://epubs.siam.org/doi/abs/10.1137/141000270" target=&ldquo;blank&rdquo;>An Accelerated Randomized Proximal Coordinate Gradient Method and its Application to Regularized Empirical Risk Minimization</a> <br />
Qihang Lin, Zhaosong Lu, and Lin Xiao <br />
<i>SIAM Journal on Optimization</i>, 25(4):2244–2273, 2015.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1305.4723" target=&ldquo;blank&rdquo;>On the Complexity Analysis of Randomized Block-Coordinate Descent Methods</a> <br />
Zhaosong Lu and Lin Xiao <br />
<i>Mathematical Programming</i>, Series A, 152(1-2):15-642, 2015.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://link.springer.com/article/10.1007/s10589-014-9694-4" target=&ldquo;blank&rdquo;>An adaptive accelerated proximal gradient method and its homotopy continuation for sparse optimization</a> <br />
Qihang Lin and Lin Xiao <br />
<i>Computational Optimization and Applications</i>, 60(3):633-674, 2015.
</p>
</li>
</ul>
<ul>
<li><p><a href="https://papers.nips.cc/paper/2015/hash/4ca82782c5372a547c104929f03fe7a9-Abstract.html" target=&ldquo;blank&rdquo;>End-to-end Learning of LDA by Mirror-Descent Back Propagation over a Deep Architecture</a> <br />
Jianshu Chen, Ji He, Yelong Shen, Lin Xiao, Xiaodong He, Jianfeng Gao, Xinying Song, Li Deng <br />
<i>Advances in Neural Information Processing Systems 28</i> (NIPS 2015).
</p>
</li>
</ul>
<ul>
<li><p><a href="https://dl.acm.org/doi/10.1145/2783258.2783412" target=&ldquo;blank&rdquo;>Scaling Up Stochastic Dual Coordinate Ascent</a> <br />
Kenneth Tran, Saghar Hosseini, Lin Xiao, Thomas Finley, Mikhail Bilenko <br />
<i>Proceedings of the 21st ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)</i>, 2015.
</p>
</li>
</ul>
<ul>
<li><p><a href="http://proceedings.mlr.press/v37/zhanga15.html" target=&ldquo;blank&rdquo;>Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization</a> <br />
Yuchen Zhang and Lin Xiao <br />
<i>Proceedings of the 32nd International Conference on Machine Learning</i>, PMLR 37:353-361, 2015.
</p>
</li>
</ul>
<ul>
<li><p><a href="http://proceedings.mlr.press/v37/zhangb15.html" target=&ldquo;blank&rdquo;>DiSCO: Distributed Optimization for Self-Concordant Empirical Loss</a> <br />
Yuchen Zhang and Lin Xiao <br />
<i>Proceedings of the 32nd International Conference on Machine Learning</i>, PMLR 37:362-370, 2015.
</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2026-01-02 23:23:11 PST, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
